# -*- coding: utf-8 -*-
"""logistic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-V_fa9xmlQtI73bRerajR1eCrf1Nh8a1

## Agenda
#### 1.Refresh sigmoid function and use in logistic regression.
#### 2. Brief overview of probability, odds, e, log, and log-odds
#### 3. Data cleaning
#### 4. Feature selection for linear models
#### 5. Feature Engineering and feature encoding
#### 6. Implement logistic regression using sklearn and stasmodel.api
#### 7. Evaluation metrics

# Logistic regression
Instead of fitting a line to the raw 1s and 0s of the data, logistic regression models the _probability_ that the outcome is 1 given the value of the predictor. Since probabilities have to be between 0 and 1, to do this we need a function that transforms our predictor variable (which in theory could range from −∞ to +∞) to a value between 0 and 1. Lots of functions can do that, but the logistic function is the most appropriate choice when dealing with probabilities.

$$\textrm{theLogisticFunction}\thinspace(t) = \frac{1}{1 + e^{-t}}$$

In R, we implement this as `1 / (1 + exp(-t))` where `t` is our continuous predictor variable. We can apply the logistic function to transform the right side of the OLS regression equation, and use it to model the _probability_ of happiness instead of the raw `is_happy` value, as seen here:

$$P(\textrm{is_happy}) = \textrm{theLogisticFunction}(\textrm{intercept} + \textrm{slope} \times \textrm{beta})$$

In practice, the transformation is actually achieved by applying the “inverse logistic function” (aka, the “logit” function: $\textrm{logit}(p) = log(\frac{p}{1-p})$) to the left side of the equation (instead of applying the logistic function to the right side):

$$\textrm{logit}(P(\textrm{is_happy})) = \textrm{intercept} + \textrm{slope} \times \textrm{beta}$$

## Probability, odds, and log-odds
The left-hand side of the above equation, with the form $\textrm{logit}(P(\textrm{some_binary_outcome}))$ is also known as the “log-odds”. In gambling contexts, odds are usually given as “five-to-four in favor”; in statistics the meaning of odds is the same (expected number of “yes” results divided by expected number of “no” results), but the odds are reduced to a single number ($\textrm{“5-to-4”} = \frac{5}{4} = 1.25$) and “log-odds” is just $log(\textrm{odds})$. We can easily convert back and forth among log-odds, odds, and probability:

$$\begin{alignat}{3}
\textrm{odds} & = e^{\textrm{log odds}} && = \frac{\textrm{probability}}{1 - \textrm{probability}} \\
\textrm{log odds} & = log(\textrm{odds}) && = log(\frac{\textrm{probability}}{1 - \textrm{probability}}) \\
\textrm{probability} & = \frac{\textrm{odds}}{1 + \textrm{odds}} && = \frac{e^{\textrm{log odds}}}{1 + e^{\textrm{log odds}}}
\end{alignat}$$
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import zipfile
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline


# Plotly visualizations
from plotly import tools
import plotly.plotly as py
import plotly.figure_factory as ff
import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
init_notebook_mode(connected=True)
import statsmodels.api as sm

import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

zip_file = zipfile.ZipFile('/content/drive/My Drive/lending-club-loan-data.zip')
data = pd.read_csv(zip_file.open('loan.csv'), sep=",", error_bad_lines=False, low_memory=False,
                   parse_dates=["issue_d", "earliest_cr_line", "last_pymnt_d",
                                "next_pymnt_d", "last_credit_pull_d"])
data.head()

data = pd.read_csv("./loan.csv", sep=",", error_bad_lines=False, low_memory=False,
                   parse_dates=["issue_d", "earliest_cr_line", "last_pymnt_d",
                                "next_pymnt_d", "last_credit_pull_d"])
data.head()

data.shape

"""KNOW ABOUT YOUR DATA"""

data.info()

"""PRE-PROCESSING


NULL VALUES:
"""

data.isnull().sum()

"""#### Unfortunately there are columns where most of values are NULLs. They are completely useless, so just remove those columns where more than 1% of the rows for that column contain a null value."""

cleaned_data = data[[label for label in data if data[label].isnull().sum() <= 0.01 * data.shape[0]]]

cleaned_data.isnull().sum()

"""#### Ok, much better. Now we have to do something with those NULL values. We can:
remove rows cointain NULL values,
fill them with median or mode value,
or use some imputation and try to predict their missing values.
Let's try first option and see what will happen.
"""

cleaned_data = cleaned_data.dropna()
float(cleaned_data.shape[0]) / data.shape[0]

"""#### It looks good, we removed less than 1% of rows. I think it's good enough and there's no point to do something more with that.

### Remove useless columns

#### lets take a look at data 

#### As we can see, two first columns contain randomly generated numbers, which are some identifiers. The column "url" also contains information about id,
#### What's more, "zip_code" is redundant with the "addr_state" and only 3 digits of 5 digit code zip are visible,
#### According to: https://www.lendingclub.com/public/rates-and-fees.action column sub_grade is reduntant to columns "grade" and "int_rate",
#### "title" column requires a lot of processing to become usefull.
"""

cleaned_data = cleaned_data.drop(["sub_grade", "zip_code"], axis=1)
cleaned_data.head()

"""## Columns with only one value
#### Or not only one, but with values which have insignificant frequencies.
"""

for label in list(cleaned_data):
    if len(cleaned_data[label].unique()) < 5:
        print(cleaned_data[label].value_counts())
        print("\n")

"""We can see that feature "pymnt_plan" has only two possible values: "n" and "y", but with only 10 occurrences of "y" (less than 1%), so definitely it is insignificant. The same with "application_type" feature: value "joint" has 0,05% frequency. On the other hand, feature "policy_code" has only one possible value, so it's absolutely useless for us."""

cleaned_data = cleaned_data.drop(["pymnt_plan", "policy_code", "application_type", "acc_now_delinq","hardship_flag","debt_settlement_flag"],
                                 axis=1)
cleaned_data.head()

cleaned_data.to_csv('./cleaned_output.csv', encoding='utf-8')

cleaned_data.columns

"""### Numerical Variables And Categorical variables"""

numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
print(cleaned_data.select_dtypes(include=numerics).columns)
print('\n')
print(cleaned_data.select_dtypes(exclude=numerics).columns)

# cleaned_data.replace('n/a', np.nan,inplace=True)
# cleaned_data.emp_length.fillna(value=0,inplace=True)

# cleaned_data['emp_length'].replace(to_replace='[^0-9]+', value='', inplace=True, regex=True)
# cleaned_data['emp_length'] = cleaned_data['emp_length'].astype(int)

cleaned_data['term'] = cleaned_data['term'].apply(lambda x: x.lstrip())

cleaned_data['year'] = cleaned_data.issue_d.dt.year

# Determining the loans that are bad from loan_status column

bad_loan = ["Charged Off", "Default", "Does not meet the credit policy. Status:Charged Off", "In Grace Period", 
            "Late (16-30 days)", "Late (31-120 days)"]


cleaned_data['loan_condition'] = np.nan

def loan_condition(status):
    if status in bad_loan:
        return 'Bad Loan'
    else:
        return 'Good Loan'
    
    
cleaned_data['loan_condition'] = cleaned_data['loan_status'].apply(loan_condition)

f, ax = plt.subplots(1,2, figsize=(16,8))

colors = ["#3791D7", "#D72626"]
labels ="Good Loans", "Bad Loans"

plt.suptitle('Information on Loan Conditions', fontsize=20)

cleaned_data["loan_condition"].value_counts().plot.pie(explode=[0,0.25], autopct='%1.2f%%', ax=ax[0], shadow=True, colors=colors, 
                                             labels=labels, fontsize=12, startangle=70)


# ax[0].set_title('State of Loan', fontsize=16)
ax[0].set_ylabel('% of Condition of Loans', fontsize=14)

# sns.countplot('loan_condition', data=df, ax=ax[1], palette=colors)
# ax[1].set_title('Condition of Loans', fontsize=20)
# ax[1].set_xticklabels(['Good', 'Bad'], rotation='horizontal')
palette = ["#3791D7", "#E01E1B"]

sns.barplot(x="year", y="loan_amnt", hue="loan_condition", data=cleaned_data, palette=palette, estimator=lambda x: len(x) / len(cleaned_data) * 100)
ax[1].set(ylabel="(%)")

"""### Clearly the data is totally unbalanced!! 

#### This is a clear example where using a typical accuracy score to evaluate our classification algorithm. For example, if we just used a majority class to assign values to all records, we will still be having a high accuracy, BUT WE WOULD BE CLASSIFYING ALL "1" INCORRECTLY!!

#### There are several ways to approach this classification problem taking into consideration this unbalance. 

- Collect more data? Nice strategy but not applicable in this case
- Changing the performance metric:
    - Use the confusio nmatrix to calculate Precision, Recall
    - F1score (weighted average of precision recall)
    - Use Kappa - which is a classification accuracy normalized by the imbalance of the classes in the data
    - ROC curves - calculates sensitivity/specificity ratio.
    
- Resampling the dataset
    - Essentially this is a method that will process the data to have an approximate 50-50 ratio.
    - One way to achieve this is by OVER-sampling, which is adding copies of the under-represented class (better when you have little data)
    - Another is UNDER-sampling, which deletes instances from the over-represented class (better when he have lot's of data)
"""

fig, ax = plt.subplots(1, 3, figsize=(16,5))
loan_amount = cleaned_data["loan_amnt"].values
funded_amount = cleaned_data["funded_amnt"].values
investor_funds = cleaned_data["funded_amnt_inv"].values


sns.distplot(loan_amount, ax=ax[0], color="#F7522F")
ax[0].set_title("Loan Applied by the Borrower", fontsize=14)
sns.distplot(funded_amount, ax=ax[1], color="#2F8FF7")
ax[1].set_title("Amount Funded by the Lender", fontsize=14)
sns.distplot(investor_funds, ax=ax[2], color="#2EAD46")
ax[2].set_title("Total committed by Investors", fontsize=14)

"""### As the distribution of above plots looks similar, it reveals that they might be correlated highly"""

# The year of 2015 was the year were the highest amount of loans were issued 
# This is an indication that the economy is quiet recovering itself.
plt.figure(figsize=(12,8))
sns.barplot('year', 'loan_amnt', data=cleaned_data, palette='tab10')
plt.title('Issuance of Loans', fontsize=16)
plt.xlabel('Year', fontsize=14)
plt.ylabel('Average loan amount issued', fontsize=14)

cleaned_data['addr_state'].unique()

# Make a list with each of the regions by state.

west = ['CA', 'OR', 'UT','WA', 'CO', 'NV', 'AK', 'MT', 'HI', 'WY', 'ID']
south_west = ['AZ', 'TX', 'NM', 'OK']
south_east = ['GA', 'NC', 'VA', 'FL', 'KY', 'SC', 'LA', 'AL', 'WV', 'DC', 'AR', 'DE', 'MS', 'TN' ]
mid_west = ['IL', 'MO', 'MN', 'OH', 'WI', 'KS', 'MI', 'SD', 'IA', 'NE', 'IN', 'ND']
north_east = ['CT', 'NY', 'PA', 'NJ', 'RI','MA', 'MD', 'VT', 'NH', 'ME']

cleaned_data['region'] = np.nan

def finding_regions(state):
    if state in west:
        return 'West'
    elif state in south_west:
        return 'SouthWest'
    elif state in south_east:
        return 'SouthEast'
    elif state in mid_west:
        return 'MidWest'
    elif state in north_east:
        return 'NorthEast'

cleaned_data['region'] = cleaned_data['addr_state'].apply(finding_regions)
cleaned_data.drop(['addr_state'], axis =1 , inplace=True)

"""## Datetime features
### We have datatype columns, but now it's useless - we have to extract some information from them.
"""

print(cleaned_data.select_dtypes(include=["datetime"]).head())

datetime_features = list(cleaned_data.select_dtypes(include=["datetime"]))

for label in datetime_features:
    month_year = (cleaned_data[label].dt.month + cleaned_data[label].dt.year * 100)
    month_year_cnt_map = month_year.value_counts().to_dict()
    cleaned_data[label + "_month_year_cnt"] = month_year.map(month_year_cnt_map)
    
    cleaned_data[label + "_year"] = cleaned_data[label].dt.year
    cleaned_data[label + "_month"] = cleaned_data[label].dt.month

cleaned_data = cleaned_data.drop(datetime_features, axis=1)

"""### Feature Engineering"""

cleaned_data['amt_difference'] = 'eq'
cleaned_data.loc[(cleaned_data['funded_amnt'] - cleaned_data['funded_amnt_inv']) > 0,'amt_difference'] = 'less'

"""#### * the metrics `delinq_2yrs` is very skewed towards zero (80% are zeros). Let's make it categorical: `no` when `delinq_2yrs == 0` and `yes` when  `delinq_2yrs > 0`

#### * Same as above for `inq_last_6mths`: The number of inquiries in past 6 months (excluding auto and mortgage inquiries)

#### * Same as above for `pub_rec`: Number of derogatory public records
"""

# Make categorical

cleaned_data['delinq_2yrs_cat'] = 'no'
cleaned_data.loc[cleaned_data['delinq_2yrs']> 0,'delinq_2yrs_cat'] = 'yes'

cleaned_data['inq_last_6mths_cat'] = 'no'
cleaned_data.loc[cleaned_data['inq_last_6mths']> 0,'inq_last_6mths_cat'] = 'yes'

cleaned_data['pub_rec_cat'] = 'no'
cleaned_data.loc[cleaned_data['pub_rec']> 0,'pub_rec_cat'] = 'yes'

# Create new metric
cleaned_data['acc_ratio'] = cleaned_data.open_acc / cleaned_data.total_acc

"""### Correlation between numerical variables"""

cleaned_data.drop(['year'], axis=1)
f, ax = plt.subplots(figsize=(15, 10))
corr = cleaned_data.corr()
corr_df = pd.DataFrame(corr)
sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),
            square=True, ax=ax)

corr_df

"""### Drop highly correlated features"""

# Select upper triangle of correlation matrix
upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))

# Find index of feature columns with correlation greater than 0.95
to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]
print(to_drop)

"""### Convert categorical variables into numerical variables"""

cleaned_data["initial_list_status"] = cleaned_data["initial_list_status"].map({"f": 1, "w": 0})
cleaned_data["loan_condition"] = cleaned_data["loan_condition"].map({"Good Loan":0, "Bad Loan":1})
to_drop.extend(["loan_status"])
print('bla:',to_drop)
cleaned_data.drop(labels=to_drop, axis=1, inplace=True)
# cleaned_data = pd.get_dummies(cleaned_data, columns=list(cleaned_data.select_dtypes(include=["object"])))

features = ['loan_amnt', 'amt_difference', 'term','grade',
            'home_ownership', 'annual_inc','verification_status',
            'purpose', 'dti', 'delinq_2yrs_cat', 'inq_last_6mths_cat', 
            'open_acc', 'pub_rec', 'pub_rec_cat', 'acc_ratio', 'initial_list_status',  
            'loan_condition','int_rate','region','year','issue_d_month'
           ]
cleaned_data_sub = cleaned_data[features]

cleaned_data_sub = pd.get_dummies(cleaned_data_sub, columns=list(cleaned_data_sub.select_dtypes(include=["object"])))

"""### Train and test split"""

X = cleaned_data_sub.ix[:, cleaned_data_sub.columns != "loan_condition"]
y = cleaned_data_sub.ix[:, cleaned_data_sub.columns == "loan_condition"]

from sklearn.model_selection import train_test_split

# Whole dataset
X_Train, X_Test, y_Train, y_Test = train_test_split(X,y,test_size = 0.3, random_state = 0)

print("Number transactions train dataset: ", len(X_Train))
print("Number transactions test dataset: ", len(X_Test))
print("Total number of transactions: ", len(X_Train)+len(X_Test))

logit_model = sm.Logit(y_Train.values.ravel(),X_Train)
result = logit_model.fit(maxiter=45)
print(result.summary())

"""### Logistic Regression Model Fitting"""

from sklearn.linear_model import LogisticRegression
from sklearn import metrics
logreg = LogisticRegression()
logreg.fit(X_Train, y_Train)

coeff = list(logreg.coef_[0])
labels = list(X_Train.columns)
features = pd.DataFrame()
features['Features'] = labels
features['importance'] = coeff
features.sort_values(by=['importance'], ascending=True, inplace=True)
features['positive'] = features['importance'] > 0
features.set_index('Features', inplace=True)
features.importance.plot(kind='barh', figsize=(10,15),color = features.positive.map({True: 'blue', False: 'red'}))
plt.xlabel('Importance')

y_pred = logreg.predict(X_Test)

print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_Test, y_Test)))

from sklearn.metrics import classification_report
print(classification_report(y_Test, y_pred))

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
logit_roc_auc = roc_auc_score(y_Test, logreg.predict(X_Test))
fpr, tpr, thresholds = roc_curve(y_Test, logreg.predict_proba(X_Test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()

"""###  Confusion matrix"""

from sklearn.metrics import confusion_matrix
import itertools

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=0)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        #print("Normalized confusion matrix")
    else:
        1#print('Confusion matrix, without normalization')

    #print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

# Compute confusion matrix
cnf_matrix = confusion_matrix(y_Test,y_pred)

# Plot non-normalized confusion matrix
class_names = [0,1]
plt.figure()
plot_confusion_matrix(cnf_matrix
                      , classes=class_names
                      , title='Confusion matrix')
plt.show()